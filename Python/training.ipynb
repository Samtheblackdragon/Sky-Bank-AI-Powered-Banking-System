{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np \n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "intents = json.loads(open('intents.json').read())\n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_letters = [\"?\",\"!\",\".\",\",\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['Hello'], 'greetings'), (['Hi'], 'greetings'), (['Hey'], 'greetings'), (['Howdy'], 'greetings'), (['How', 'are', 'you', '?'], 'greetings'), (['What', \"'s\", 'up', '?'], 'greetings'), (['Good', 'morning'], 'greetings'), (['Wake', 'up', '!'], 'greetings'), (['How', 'is', 'it', 'going', '?'], 'greetings'), (['Bye'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Shut', 'up'], 'goodbye'), (['Do', \"n't\", 'talk'], 'goodbye'), (['Goodbye'], 'goodbye'), (['Have', 'a', 'nice', 'day'], 'goodbye'), (['I', 'quit'], 'goodbye'), (['What', 'is', 'your', 'name', '?'], 'name'), (['What', 'can', 'I', 'call', 'you', '?'], 'name'), (['Can', 'you', 'tell', 'me', 'your', 'name', '?'], 'name'), (['Who', 'are', 'you', '?'], 'name'), (['Please', 'tell', 'me', 'your', 'name'], 'name'), (['Can', 'you', 'help', 'me', 'with', 'the', 'transaction', '?'], 'transact'), (['How', 'to', 'transact', 'money', '?'], 'transact'), (['How', 'to', 'transfer', 'money', 'to', 'someone', \"'s\", 'account', '?'], 'transact'), (['Where', 'is', 'transaction', 'tab', '?'], 'transact'), (['Want', 'to', 'send', 'money'], 'transact'), (['Transact', 'money'], 'transact'), (['Can', 'you', 'help', 'me', 'with', 'the', 'deposit', '?'], 'deposit'), (['How', 'to', 'deposit', 'money', '?'], 'deposit'), (['How', 'to', 'deposit', 'money', 'into', 'my', 'account', '?'], 'deposit'), (['Deposit', 'money'], 'deposit'), (['Where', 'is', 'deposit', 'tab', '?'], 'deposit'), (['Want', 'to', 'deposit', 'money'], 'deposit'), (['How', 'is', 'the', 'graph', 'generated'], 'graph'), (['Is', 'the', 'graph', 'accurate', '?'], 'graph'), (['What', 'does', 'the', 'graph', 'do', '?'], 'graph'), (['Why', 'is', 'my', 'graph', 'not', 'showing', 'anything', '?'], 'graph'), (['How', 'is', 'the', 'pie', 'chart', 'generated'], 'pie'), (['Is', 'the', 'pie', 'char', 'accurate', '?'], 'pie'), (['What', 'does', 'the', 'pie', 'char', 'do', '?'], 'pie'), (['Why', 'is', 'my', 'pie', 'chart', 'blank', '?'], 'pie'), (['Where', 'is', 'statement', 'tab', '?'], 'statement'), (['How', 'to', 'see', 'my', 'transaction', 'history', '?'], 'statement'), (['My', 'history'], 'statement'), (['Show', 'transacted', 'money', 'details'], 'statement'), (['What', 'is', 'loan', 'tools', '?'], 'tools'), (['How', 'can', 'loan', 'tools', 'help', 'me', '?'], 'tools'), (['Can', 'loan', 'predictor', 'really', 'predict', '100', '%'], 'tools'), (['Can', 'E-cal', 'really', 'breakdown', 'my', 'loan', 'in', 'emi', \"'s\", '?'], 'tools'), (['Is', 'it', 'really', '100', '%', 'accurate', '?'], 'tools'), (['Can', 'you', 'do', 'math', '?'], 'random'), (['Can', 'you', 'tell', 'me', 'your', 'code', '?'], 'random'), (['Whose', 'your', 'creator', '?'], 'random'), (['What', 'did', 'come', 'first', 'egg', 'or', 'human'], 'random'), (['What', 'is', 'your', 'gender'], 'random'), (['Are', 'you', 'single'], 'random')]\n"
     ]
    }
   ],
   "source": [
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)\n",
    "        documents.append((word_list,intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n",
    "words = sorted(set(words))\n",
    "\n",
    "classes = sorted(set(classes))\n",
    "\n",
    "pickle.dump(words, open('words.pkl','wb'))\n",
    "pickle.dump(classes,open('classes.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "for document in documents:\n",
    "    bag = []\n",
    "    word_patterns = document[0]\n",
    "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "\n",
    "    for word in words:\n",
    "        bag.append(1) if word in word_patterns else bag.append(0)\n",
    "    \n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(document[1])] = 1\n",
    "    training.append([bag,output_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sambl\\AppData\\Local\\Temp\\ipykernel_58024\\4024136173.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training = np.array(training)\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128,input_shape=(len(train_x[0]),),activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: chatbotmodel.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: chatbotmodel.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(learning_rate=0.01,weight_decay=1e-6, momentum=0.9,nesterov=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5,verbose=None)\n",
    "model.save('chatbotmodel.model')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=None)\n",
    "model.save('chatbotmodel.h5',hist)\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
